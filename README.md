# BERT-analysis
This is the homework repository of Deep Learning for Human Language Processing 
## Goal
The goal for this homework is:
* train a chinese nli task and save model 
* generate dataset which created from each layer embedding of each data in xnli sample data (two dataset: one is created from pretrained model, the other one is fintuned model.)
* analysis embeddings (anisotropy, self-similarity, Maximum Eigenvalue Variances) (adjusted version)

## Dataset: Xnli zh

## Step 1 Train xnli and achieve accuracy performance approximately 73-76%
Hint: Utilize run_xnli.py

## Step 2 Generate pretrained data and finetune data for xnli-sample data
Hint: Modify a little bit xnli python code.

## Step 3 analysis bert layer embedding:
